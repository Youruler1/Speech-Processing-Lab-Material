{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youruler1/Speech-Processing-Lab-Material/blob/main/Lab5_Automatic_Speech_Recognition_using_a_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this assignment is to develop an Automatic Speech Recognition (ASR) system using a public speech dataset and a deep learning-based model. You will preprocess the dataset, train a speech-to-text model, and evaluate its performance."
      ],
      "metadata": {
        "id": "JtN06d2Ov2Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torch"
      ],
      "metadata": {
        "id": "8EdE7DvFv7CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pre-trained model and processor"
      ],
      "metadata": {
        "id": "_su8GWWuwCNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYh7Ex2qvx1C",
        "outputId": "6938fe10-bf35-4824-f5ca-6a845d539ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wav2Vec2ForCTC(\n",
              "  (wav2vec2): Wav2Vec2Model(\n",
              "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
              "      (conv_layers): ModuleList(\n",
              "        (0): Wav2Vec2GroupNormConvLayer(\n",
              "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "        )\n",
              "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (feature_projection): Wav2Vec2FeatureProjection(\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): Wav2Vec2Encoder(\n",
              "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "        (conv): ParametrizedConv1d(\n",
              "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "          (parametrizations): ModuleDict(\n",
              "            (weight): ParametrizationList(\n",
              "              (0): _WeightNorm()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (padding): Wav2Vec2SamePadLayer()\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2SdpaAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model_name = \"facebook/wav2vec2-large-960h\"\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess an audio file"
      ],
      "metadata": {
        "id": "5AhqKvOPwIpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and process audio\n",
        "def load_audio(file_path):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    # Resample if needed\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Convert stereo to mono if needed\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0)\n",
        "\n",
        "    return waveform.squeeze()"
      ],
      "metadata": {
        "id": "rr-zTT8jwJSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert speech to text"
      ],
      "metadata": {
        "id": "BLKUCC24wN9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to transcribe audio\n",
        "def transcribe(audio_file):\n",
        "    waveform = load_audio(audio_file)\n",
        "\n",
        "    # Ensure correct shape: [1, sequence_length]\n",
        "    waveform = waveform.unsqueeze(0)  # Add batch dimension -> Shape: [1, sequence_length]\n",
        "\n",
        "    # Process the input\n",
        "    input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "    # Ensure correct shape before feeding into the model\n",
        "    input_values = input_values.squeeze(1)  # Shape: [batch_size, sequence_length]\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Decode the predicted IDs\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "    return transcription"
      ],
      "metadata": {
        "id": "J-DmJmeywOd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = torchaudio.load(audio_file_path)\n"
      ],
      "metadata": {
        "id": "88kwn8KBVfp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeZLaGg5WxaM",
        "outputId": "b1807891-5734-4a5c-886d-1adce0101e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform = torch.mean(waveform, dim=0)"
      ],
      "metadata": {
        "id": "gzfqP3NZVkYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform.unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgYZsvGfVvL_",
        "outputId": "1c9be729-d95c-407e-c44a-90ac7aefd681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZhPJsh4Vyng",
        "outputId": "534388d5-de85-4ce5-c5b5-ee7ca546fc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35316])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with an audio sample"
      ],
      "metadata": {
        "id": "AjTsbielwU0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = 'apple10.wav'  # Replace with your test audio file\n",
        "text_output = transcribe(audio_file_path)\n",
        "print(\"Transcription:\", text_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eug9R0nLwYoT",
        "outputId": "920f3294-7fe7-4806-87fa-e64fbdd37309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription: APPLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchaudio.datasets import LIBRISPEECH\n",
        "test_dataset = LIBRISPEECH(root=\"./\", url=\"test-clean\", download=True)\n",
        "\n",
        "# Evaluate on a subset (e.g., first 20 samples)\n",
        "num_samples = 20\n",
        "ground_truths = []\n",
        "predictions = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "    waveform, sample_rate, transcript, _, _, _ = test_dataset[i]\n",
        "\n",
        "    # Process and transcribe\n",
        "    waveform = load_audio(waveform, sample_rate)\n",
        "    predicted_text = transcribe(waveform)\n",
        "\n",
        "    # Store ground truth and predictions\n",
        "    ground_truths.append(transcript.lower())\n",
        "    predictions.append(predicted_text)\n",
        "\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Ground Truth: {transcript}\")\n",
        "    print(f\"Predicted: {predicted_text}\")\n",
        "\n",
        "# Compute Word Error Rate (WER)\n",
        "wer = wer_metric.compute(predictions=predictions, references=ground_truths)\n",
        "print(f\"Word Error Rate (WER): {wer:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "wTuYj28TYTis",
        "outputId": "a86eb700-f9bc-401f-efe0-2a6c226b81ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 331M/331M [00:18<00:00, 18.6MB/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "load_audio() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-597d73c7da9b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Process and transcribe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpredicted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load_audio() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    }
  ]
}