{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youruler1/Speech-Processing-Lab-Material/blob/main/ASR_Pipeline_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "from torchaudio.transforms import MelSpectrogram, Resample\n",
        "import torchaudio.functional as F\n",
        "import os\n",
        "\n",
        "\n",
        "train_dataset = LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)"
      ],
      "metadata": {
        "id": "bxzLvWeaUomG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "18a350d5-d640-44d8-b38a-77eff8b87bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.95G/5.95G [03:07<00:00, 34.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "CHAR_VOCAB = list(string.ascii_lowercase + \" '\")  # space and apostrophe included\n",
        "CHAR2IDX = {c: i + 1 for i, c in enumerate(CHAR_VOCAB)}  # index 0 reserved for padding\n",
        "IDX2CHAR = {i: c for c, i in CHAR2IDX.items()}\n",
        "BLANK_IDX = 0\n"
      ],
      "metadata": {
        "id": "yB3keBGce5mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "mel_spec = T.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80)\n",
        "amplitude_to_db = T.AmplitudeToDB()\n",
        "\n",
        "def text_to_indices(text):\n",
        "    return [CHAR2IDX[c] for c in text.lower() if c in CHAR2IDX]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    waveforms, labels = [], []\n",
        "    input_lengths, label_lengths = [], []\n",
        "\n",
        "    for waveform, _, transcript,_ , _, _ in batch:\n",
        "        features = amplitude_to_db(mel_spec(waveform)).squeeze(0).transpose(0, 1)  # (time, features)\n",
        "        waveforms.append(features)\n",
        "        label_seq = torch.tensor(text_to_indices(transcript), dtype=torch.long)\n",
        "        labels.append(label_seq)\n",
        "        input_lengths.append(features.shape[0])\n",
        "        label_lengths.append(len(label_seq))\n",
        "\n",
        "    waveforms = pad_sequence(waveforms, batch_first=True)\n",
        "    labels = pad_sequence(labels, batch_first=True)\n",
        "    return waveforms, labels, torch.tensor(input_lengths), torch.tensor(label_lengths)\n"
      ],
      "metadata": {
        "id": "7agMsr5Se8ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "ndaLfA1eeupM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNNLSTM_ASR(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bidirectional\n",
        "\n",
        "    def forward(self, x, input_lengths):\n",
        "        # x shape: (batch, time, features) → CNN expects (batch, features, time)\n",
        "        x = x.transpose(1, 2)  # (B, F, T)\n",
        "        x = self.cnn(x)\n",
        "        x = x.transpose(1, 2)  # (B, T, F)\n",
        "\n",
        "        # Pack padded sequences for LSTM\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_outputs, _ = self.lstm(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        logits = self.fc(outputs)  # (B, T, Vocab)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "1Wbm7-NtfNCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "VOCAB_SIZE = len(CHAR2IDX) + 1  # +1 for blank token\n",
        "\n",
        "model = CNNLSTM_ASR(input_dim=80, hidden_dim=256, output_dim=VOCAB_SIZE).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "ctc_loss = nn.CTCLoss(blank=BLANK_IDX, zero_infinity=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "xvAjrNM6_bpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        inputs, targets, input_lengths, target_lengths = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        logits = model(inputs, input_lengths)\n",
        "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
        "        log_probs = log_probs.transpose(0, 1)  # CTC expects (T, B, C)\n",
        "\n",
        "        loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "WCZCz6GR_eZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}