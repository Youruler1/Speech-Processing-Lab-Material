{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youruler1/Speech-Processing-Lab-Material/blob/main/wav2vec1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wav2vec1.0 (Scratch Implementation)\n",
        "\n"
      ],
      "metadata": {
        "id": "8dSg-kFYDIfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---- Step 1: Feature Encoder (Temporal CNN) ----\n",
        "class FeatureEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=512):\n",
        "        super(FeatureEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=10, stride=5, padding=2)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=8, stride=4, padding=2)\n",
        "        self.conv3 = nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv1d(512, hidden_dim, kernel_size=4, stride=2, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.norm = nn.LayerNorm(hidden_dim)  # Layer Normalization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = x.permute(0, 2, 1)  # (B, T', C) for layer_norm\n",
        "        x = self.norm(x)\n",
        "        return x  # Feature embeddings\n",
        "\n",
        "# ---- Step 2: Context Network (Deeper Temporal CNN) ----\n",
        "class ContextNetwork(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, num_layers=9):\n",
        "        super(ContextNetwork, self).__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Convert to (B, C, T')\n",
        "        for conv in self.convs:\n",
        "            x = self.relu(conv(x))\n",
        "        return x.permute(0, 2, 1)  # Back to (B, T', C)\n",
        "\n",
        "# ---- Step 3: Contrastive Loss ----\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.1):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z, z_pos, z_neg):\n",
        "        \"\"\"\n",
        "        z: Anchor representations (B, T', C)\n",
        "        z_pos: Positive representations (B, T', C)\n",
        "        z_neg: Negative samples (B, T', C)\n",
        "        \"\"\"\n",
        "        z = F.normalize(z, dim=-1)\n",
        "        z_pos = F.normalize(z_pos, dim=-1)\n",
        "        z_neg = F.normalize(z_neg, dim=-1)\n",
        "\n",
        "        # Compute similarity scores\n",
        "        pos_sim = (z * z_pos).sum(dim=-1) / self.temperature  # Positive similarity\n",
        "        neg_sim = (z * z_neg).sum(dim=-1) / self.temperature  # Negative similarity\n",
        "\n",
        "        # Contrastive loss: maximize difference between positive & negative\n",
        "        loss = -torch.mean(pos_sim - neg_sim)\n",
        "        return loss\n",
        "\n",
        "# ---- Step 4: Utility Function for Downsampling ----\n",
        "def downsample(x, feature_encoder):\n",
        "    \"\"\"Pass positive/negative samples through the same feature encoder to match time steps.\"\"\"\n",
        "    with torch.no_grad():  # No gradient needed for precomputed samples\n",
        "        x = feature_encoder(x)  # Now (B, T', C)\n",
        "    return x\n",
        "\n",
        "# ---- Step 5: Combine Everything into Wav2Vec 1.0 Model ----\n",
        "class Wav2Vec1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Wav2Vec1, self).__init__()\n",
        "        self.feature_encoder = FeatureEncoder()\n",
        "        self.context_network = ContextNetwork()\n",
        "        self.contrastive_loss = ContrastiveLoss()\n",
        "\n",
        "    def forward(self, x, z_pos, z_neg):\n",
        "        features = self.feature_encoder(x)   # (B, T', C)\n",
        "        z_pos = downsample(z_pos, self.feature_encoder)  # Match T'\n",
        "        z_neg = downsample(z_neg, self.feature_encoder)  # Match T'\n",
        "\n",
        "        context = self.context_network(features)  # (B, T', C)\n",
        "        loss = self.contrastive_loss(context, z_pos, z_neg)\n",
        "        return loss, context\n",
        "\n",
        "# ---- Step 6: Training on synthetic Data ----\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a random speech-like waveform (Batch size=4, Channels=1, Time=16000)\n",
        "    speech_waveform = torch.randn(4, 1, 16000)\n",
        "    positive_samples = torch.randn(4, 1, 16000)  # Augmented or real samples\n",
        "    negative_samples = torch.randn(4, 1, 16000)  # Distractor speech samples\n",
        "\n",
        "    model = Wav2Vec1()\n",
        "    loss, _ = model(speech_waveform, positive_samples, negative_samples)\n",
        "\n",
        "    print(f\"Contrastive Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7YA0TCJynO0",
        "outputId": "ec5472c4-87f6-4342-85fe-8038122949f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrastive Loss: 0.011717967689037323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Speech Audio Dataset"
      ],
      "metadata": {
        "id": "BLSPW__qDRG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchaudio\n",
        "\n",
        "# Create the 'data' directory if it doesn't exist\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "\n",
        "# Now, download the dataset\n",
        "dataset = torchaudio.datasets.SPEECHCOMMANDS(root=\"./data\", download=True)\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "\n",
        "waveform, sample_rate, label, *_ = dataset[0]\n",
        "print(f\"Waveform shape: {waveform.shape}, Label: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxXLd1c5t-Y4",
        "outputId": "8d294071-6cfa-4683-9498-8e8dc02ffeb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:24<00:00, 101MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 105829\n",
            "Waveform shape: torch.Size([1, 16000]), Label: backward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the existing wav2vec1.0 to a classifier for speech audio dataset"
      ],
      "metadata": {
        "id": "87B7V-tJDUtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Wav2VecClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Wav2VecClassifier, self).__init__()\n",
        "        self.feature_encoder = FeatureEncoder()\n",
        "        self.context_network = ContextNetwork()\n",
        "        self.fc = nn.Linear(512, num_classes)  # Classification head\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_encoder(x)\n",
        "        x = self.context_network(x)\n",
        "        x = torch.mean(x, dim=1)  # Global Average Pooling (B, C)\n",
        "        return self.fc(x)  # Logits"
      ],
      "metadata": {
        "id": "9eFYDUOzemrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tune Wav2Vec Classifier on speech audio dataset"
      ],
      "metadata": {
        "id": "Ao2Y-0r4DcDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import DataLoader # Import the DataLoader class\n",
        "def preprocess_audio(waveform, sample_rate, target_length=16000):\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
        "    if sample_rate != 16000:\n",
        "        resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "    if waveform.shape[1] < target_length:\n",
        "        pad_amount = target_length - waveform.shape[1]\n",
        "        waveform = F.pad(waveform, (0, pad_amount))\n",
        "    else:\n",
        "        waveform = waveform[:, :target_length]\n",
        "    return waveform\n",
        "\n",
        "# ---- Load Speech Commands Dataset ----\n",
        "class SpeechCommandsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, label_map):\n",
        "        self.dataset = dataset\n",
        "        self.label_map = label_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, sample_rate, label, *_ = self.dataset[idx]\n",
        "        waveform = preprocess_audio(waveform, sample_rate)\n",
        "        return waveform, self.label_map[label]\n",
        "\n",
        "# ---- Training Function ----\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss, total_correct, total_samples = 0, 0, 0\n",
        "        for waveforms, labels in train_loader:\n",
        "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(waveforms)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Accuracy={total_correct/total_samples:.4f}\")\n",
        "\n",
        "# ---- Main Execution ----\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = torchaudio.datasets.SPEECHCOMMANDS(root=\"./data\", download=True)\n",
        "\n",
        "    # Create label mapping\n",
        "    labels = sorted(set(entry[2] for entry in dataset))\n",
        "    label_map = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "    # Prepare dataset and loader\n",
        "    train_dataset = SpeechCommandsDataset(dataset, label_map)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = Wav2VecClassifier(num_classes=len(label_map)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, criterion, optimizer, num_epochs=10)"
      ],
      "metadata": {
        "id": "qOUA_xN3eVIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fdvJpHWUDFnD"
      }
    }
  ]
}